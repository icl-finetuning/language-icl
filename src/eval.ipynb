{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import string\n",
    "\n",
    "from eval import get_run_metrics, read_run_dir, get_model_from_run\n",
    "from plot_utils import basic_plot, collect_results, relevant_model_names\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')\n",
    "\n",
    "run_dir = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d018b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = read_run_dir(run_dir)\n",
    "# df  # list all the runs in our run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9980951",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"linear_regression/lang_pretrained/random/gpt2-small/\"\n",
    "\n",
    "run_id = \"4b42e1c1-0537-4b5b-8492-211352f8294e\"\n",
    "\n",
    "# model name for plots, make sure to capitalize the first letter\n",
    "model_size = \"gpt2-small\"\n",
    "# task name for plots\n",
    "task_name = \"linear regression\"\n",
    "\n",
    "run_path = os.path.join(run_dir, task, run_id)\n",
    "recompute_metrics = True\n",
    "\n",
    "if recompute_metrics:\n",
    "    # question: is this with the test set? -- i think so\n",
    "    get_run_metrics(run_path)  # these are normally precomputed at the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_current_figure(folder_path, file_name):\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "    folder_path = \"plots/\" + folder_path\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "\n",
    "def generate_random_string(length=8):\n",
    "    letters_and_digits = string.ascii_letters + string.digits\n",
    "    random_string = ''.join(random.choice(letters_and_digits) for _ in range(length))\n",
    "    return random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f48ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_output_dir = run_id + \"_\" + task + \"_\" + generate_random_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d09964",
   "metadata": {},
   "source": [
    "# Plot pre-computed metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e02c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def valid_row(r):\n",
    "    return r.task == task and r.run_id == run_id\n",
    "\n",
    "metrics = collect_results(run_dir, df, valid_row=valid_row)\n",
    "_, conf = get_model_from_run(run_path, only_conf=True)\n",
    "n_dims = conf.model.n_dims\n",
    "\n",
    "models = relevant_model_names[task]\n",
    "basic_plot(metrics[\"standard\"], models=models)\n",
    "plt.title(model_size + \" model on \" + task_name + \" in-context task squared error vs in-context examples\")\n",
    "plt.show()\n",
    "# save_current_figure(fig_output_dir, \"eval_on_all_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4ecca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot any OOD metrics, out of distribution\n",
    "for name, metric in metrics.items():\n",
    "    if name == \"standard\": continue\n",
    "   \n",
    "    if \"scale\" in name:\n",
    "        scale = float(name.split(\"=\")[-1])**2\n",
    "    else:\n",
    "        scale = 1.0\n",
    "\n",
    "    trivial = 1.0 if \"noisy\" not in name else (1+1/n_dims)\n",
    "    fig, ax = basic_plot(metric, models=models, trivial=trivial * scale)\n",
    "    ax.set_title(name)\n",
    "    \n",
    "    if \"ortho\" in name:\n",
    "        ax.set_xlim(-1, n_dims - 1)\n",
    "    ax.set_ylim(-.1 * scale, 1.5 * scale)\n",
    "\n",
    "    plt.show()\n",
    "    if (\".\" in name):\n",
    "        name = name.replace(\".\", \"\")\n",
    "    # save_current_figure(fig_output_dir, \"eval_on_all_models_ood_\" + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f961d4",
   "metadata": {},
   "source": [
    "# Interactive setup\n",
    "\n",
    "We will now directly load the model and measure its in-context learning ability on a batch of random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb327ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from samplers import get_data_sampler\n",
    "from tasks import get_task_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03523b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, conf = get_model_from_run(run_path)\n",
    "\n",
    "n_dims = conf.model.n_dims\n",
    "batch_size = conf.training.batch_size\n",
    "\n",
    "data_sampler = get_data_sampler(conf.training.data, n_dims)\n",
    "task_sampler = get_task_sampler(\n",
    "    conf.training.task,\n",
    "    n_dims,\n",
    "    batch_size,\n",
    "    **conf.training.task_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_ys(num_changes, num_total, b_dim, y_dim=0):\n",
    "    \"\"\"\n",
    "    Selects num_changes random indices from the range 0 to num_total - 1.\n",
    "\n",
    "    Args:\n",
    "    - num_total (int): Total number of indices.\n",
    "    - num_changes (int): Number of random indices to select.\n",
    "    - b_dim (int): Dimension of a batch\n",
    "    - y_dim (int): Dimension of y\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor containing the selected random indices. batch size x num changes \n",
    "    - torch.Tensor: Tensor containing the new random values at those indices\n",
    "    \"\"\"\n",
    "\n",
    "    if num_changes > num_total:\n",
    "        raise ValueError(\"Number of changes cannot be greater than the total number.\")\n",
    "    \n",
    "    # Generate random indices\n",
    "    # torch.randperm(len(pictures))[:10]\n",
    "\n",
    "    random_indices = []\n",
    "\n",
    "    # Generate random batches\n",
    "    for _ in range(b_dim):\n",
    "        # Generate a random permutation of 0 to num_total-1, taking only the first num_changes\n",
    "        permutation = torch.range(0, num_changes - 1, 1).type(torch.int64)\n",
    "        \n",
    "        # Add the batch to the list\n",
    "        random_indices.append(permutation)\n",
    "\n",
    "    # Convert the list of batches to a PyTorch tensor\n",
    "    random_indices = torch.stack(random_indices)\n",
    "\n",
    "    if (y_dim == 0):\n",
    "        indices_values = torch.rand((b_dim, num_changes))\n",
    "    else: \n",
    "        indices_values = torch.rand((b_dim, num_changes, y_dim))\n",
    "\n",
    "    return random_indices, indices_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ = \"seq\" in task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change this to be false\n",
    "RANDOM_XS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9da7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = task_sampler()\n",
    "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
    "\n",
    "if not SEQ:\n",
    "    ys = task.evaluate(xs)\n",
    "    y_dim = 0\n",
    "    print(ys.shape)\n",
    "else: \n",
    "    xs, ys = task.generate_sequence(xs[:, 0, :], conf.model.n_positions)\n",
    "    y_dim = ys.shape[2]\n",
    "    print(ys.shape)\n",
    "\n",
    "# randomness in (64 x 11) -- num_indices of 11 will be randomized\n",
    "conf.model.n_positions = 2\n",
    "num_indices = [i for i in range(1, conf.model.n_positions)]\n",
    "randomized_ys_array = []\n",
    "randomized_xs_array = []\n",
    "\n",
    "for num_changes in num_indices:\n",
    "    if RANDOM_XS:\n",
    "        randomized_xs = xs.clone()\n",
    "        random_indices, x_indices_values = generate_random_ys(num_changes, randomized_xs.shape[1], randomized_xs.shape[0], xs.shape[2])\n",
    "\n",
    "        for b in range(randomized_xs.shape[0]):\n",
    "            randomized_xs[b][random_indices[b]] = x_indices_values[b]\n",
    "\n",
    "        randomized_xs_array.append(randomized_xs)\n",
    "\n",
    "    randomized_ys = ys.clone()\n",
    "    print(randomized_ys.shape)\n",
    "    random_indices, indices_values = generate_random_ys(num_changes, randomized_ys.shape[1], randomized_ys.shape[0], y_dim)\n",
    "    if num_changes == 1: \n",
    "        print(\"random_indices\", random_indices.shape)\n",
    "        print(\"indices_values\", indices_values.shape)\n",
    "\n",
    "    for b in range(randomized_ys.shape[0]):\n",
    "        if b == 0 and num_changes == 10:\n",
    "            print(\"before random ys\", randomized_ys[b])\n",
    "        randomized_ys[b][random_indices[b]] = indices_values[b]\n",
    "\n",
    "        if b == 0 and num_changes == 10:\n",
    "            print(random_indices[b])\n",
    "            print(\"after random ys\", randomized_ys[b])\n",
    "\n",
    "    randomized_ys_array.append(randomized_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(xs, ys)\n",
    "\n",
    "pred = pred.squeeze()\n",
    "\n",
    "print(\"pred\", pred[0][1]) # getting the 0th sequence of the batch\n",
    "\n",
    "# if not SEQ:\n",
    "randomized_pred_array = []\n",
    "for i in range(len(randomized_ys_array)):\n",
    "    with torch.no_grad():\n",
    "        if RANDOM_XS:\n",
    "            randomized_pred = model(randomized_xs_array[i], randomized_ys_array[i])\n",
    "        else:\n",
    "            randomized_pred = model(xs, randomized_ys_array[i])\n",
    "    randomized_pred = randomized_pred.squeeze()\n",
    "    randomized_pred_array.append(randomized_pred)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa97fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems like the metric is just subtracting the predictions from the ys, but im confused because are the ys not like passed in as context?\n",
    "metric = task.get_metric()\n",
    "loss = metric(pred, ys).numpy()\n",
    "randomized_loss = metric(randomized_pred, ys).numpy()\n",
    "randomized_loss_w_random = metric(randomized_pred, randomized_ys).numpy()\n",
    "\n",
    "\n",
    "sparsity = conf.training.task_kwargs.sparsity if \"sparsity\" in conf.training.task_kwargs else None\n",
    "baseline = {\n",
    "    \"linear_regression\": n_dims,\n",
    "    \"sparse_linear_regression\": sparsity,\n",
    "    \"relu_2nn_regression\": n_dims,\n",
    "    \"decision_tree\": 1,\n",
    "    \"seq_relu_2nn\": 0,\n",
    "    \"seq_linear\": 1,\n",
    "    \"seq_rec_linear\": 0,\n",
    "}[conf.training.task]\n",
    "\n",
    "plt.plot(loss.mean(axis=0), lw=2, label=\"0 y rand\")\n",
    "# plt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
    "# save_current_figure(fig_output_dir, \"eval_on_transformer\")\n",
    "\n",
    "# if (not SEQ):\n",
    "\n",
    "for n in range(1, len(randomized_pred_array), 2):\n",
    "    randomized_loss = metric(randomized_pred_array[n], ys).numpy()\n",
    "    \n",
    "    plt.plot(randomized_loss.mean(axis=0), lw=2, label=str(n + 1) + \" y rand\")\n",
    "    # plt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
    "    # plt.xlabel(\"# in-context examples\")\n",
    "    # plt.ylabel(\"squared error\")\n",
    "    # plt.legend()\n",
    "    # plt.title(model_size + \" model on \" + task_name + \" in-context task squared error vs in-context examples (with randomness)\")\n",
    "    # plt.show()\n",
    "    # save_current_figure(fig_output_dir, \"eval_on_transformer_\" + str(num_random) + \"_randomized_context\")\n",
    "\n",
    "plt.xlabel(\"# in-context examples\")\n",
    "plt.ylabel(\"squared error\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(model_size + \" model on \" + task_name + \" in-context task squared error vs in-context examples\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(randomized_loss_w_random.mean(axis=0), lw=2, label=\"Transformer with randomized context\")\n",
    "plt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
    "plt.xlabel(\"# in-context examples\")\n",
    "plt.ylabel(\"squared error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('seq_small_random.npy', 'wb') as f:\n",
    "    np.save(f, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cc3ad",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems like the metric is just subtracting the predictions from the ys, but im confused because are the ys not like passed in as context?\n",
    "metric = task.get_metric()\n",
    "loss = metric(pred, ys).numpy()\n",
    "randomized_loss = metric(randomized_pred, ys).numpy()\n",
    "randomized_loss_w_random = metric(randomized_pred, randomized_ys).numpy()\n",
    "\n",
    "sparsity = None\n",
    "baseline = {\n",
    "    \"linear_regression\": n_dims,\n",
    "    \"sparse_linear_regression\": sparsity,\n",
    "    \"relu_2nn_regression\": n_dims,\n",
    "    \"decision_tree\": 1,\n",
    "    \"seq_relu_2nn\": 0,\n",
    "    \"seq_linear\": 1,\n",
    "    \"seq_rec_linear\": 0,\n",
    "}[conf.training.task]\n",
    "\n",
    "## load losses for different models\n",
    "small_pca = np.load(\"./linreg_small_pca.npy\")\n",
    "small_wopca = np.load(\"./linreg_small_wopca.npy\")\n",
    "small_random = np.load(\"./linreg_small_random.npy\")\n",
    "\n",
    "plt.plot(small_pca.mean(axis=0), lw=2, label=\"with PCA\")\n",
    "plt.plot(small_wopca.mean(axis=0), lw=2, label=\"without PCA\")\n",
    "plt.plot(small_random.mean(axis=0), lw=2, label=\"random init\")\n",
    "\n",
    "# med_pca = np.load(\"./linreg_med_pca.npy\")\n",
    "# med_wopca = np.load(\"./linreg_med_wopca.npy\")\n",
    "# med_random = np.load(\"./linreg_med_random.npy\")\n",
    "\n",
    "# plt.plot(med_pca.mean(axis=0), lw=2, label=\"with PCA\")\n",
    "# plt.plot(med_wopca.mean(axis=0), lw=2, label=\"without PCA\")\n",
    "# plt.plot(med_random.mean(axis=0), lw=2, label=\"random init\")\n",
    "\n",
    "plt.xlabel(\"# in-context examples\")\n",
    "plt.ylabel(\"squared error\")\n",
    "plt.title(\"GPT2-small\" + \" on \" + task_name + \" in-context task\")\n",
    "\n",
    "plt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
